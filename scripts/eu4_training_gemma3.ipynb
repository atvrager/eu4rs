{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# EU4 AI Training with Gemma-3-270M\n\nTrain LoRA adapters using Google's Gemma-3 270M model - a compact but capable model trained on 6T tokens.\n\n**Why Gemma-3-270M?**\n- 270M params (similar to SmolLM2-360M but newer architecture)\n- Trained on 6 trillion tokens (more than SmolLM2)\n- 32K context window\n- Strong reasoning despite small size\n\n**One-time setup:**\n1. Accept Gemma license at https://huggingface.co/google/gemma-3-270m\n2. Create HuggingFace access token at https://huggingface.co/settings/tokens\n3. In Colab sidebar: ðŸ”‘ Secrets â†’ Add `HF_TOKEN` with your token\n4. Create a GitHub PAT at https://github.com/settings/tokens (classic, `repo` scope)\n5. Add secret: `GITHUB_TOKEN` with your PAT\n6. Create folder in Google Drive (e.g., `eu4_training/`)\n\n**Per-session:**\n1. Upload `.cpb.zip` training data to Drive\n2. Run all cells in order"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU - Gemma-3-270M fits comfortably on T4/L4\n",
    "# If you see \"No GPU\", go to Runtime â†’ Change runtime type â†’ GPU\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"âš ï¸  NO GPU DETECTED!\")\n",
    "    print(\"Go to: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "    print(\"Then re-run this cell.\")\n",
    "else:\n",
    "    !nvidia-smi\n",
    "    print(f\"âœ“ GPU available: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with HuggingFace for Gemma access\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = userdata.get(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "print(\"âœ“ Authenticated with HuggingFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo from GitHub\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "token = userdata.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "if os.path.exists(\"eu4rs\"):\n",
    "    !rm -rf eu4rs\n",
    "\n",
    "!git clone --depth 1 https://{token}@github.com/atvrager/eu4rs.git\n",
    "\n",
    "print(\"âœ“ Cloned eu4rs from GitHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Flash attention for Gemma efficiency (optional but recommended)\n",
    "!pip install -q transformers peft trl datasets pycapnp safetensors\n",
    "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available, using default\"\n",
    "\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE\n",
    "# ===========================\n",
    "\n",
    "# Path to your training data in Google Drive\n",
    "DATA_PATH = \"/content/drive/MyDrive/eu4_training/run_10yr_1.cpb.zip\"\n",
    "\n",
    "# Where to save the trained adapter\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/eu4_training/adapters/gemma3-270m-run1\"\n",
    "\n",
    "# Model: Gemma-3-270M (base model for fine-tuning)\n",
    "BASE_MODEL = \"google/gemma-3-270m\"\n",
    "\n",
    "# Training settings - MEMORY OPTIMIZED for T4 (16GB)\n",
    "# If OOM: reduce BATCH_SIZE to 2, or use gradient checkpointing\n",
    "MAX_STEPS = 10000\n",
    "BATCH_SIZE = 4  # Reduced from 8 to avoid OOM on T4\n",
    "GRAD_ACCUM = 4  # Increased to maintain effective batch size of 16\n",
    "SAVE_STEPS = 2500\n",
    "\n",
    "# Repo paths\n",
    "REPO_DIR = \"/content/eu4rs\"\n",
    "SCRIPTS_DIR = f\"{REPO_DIR}/scripts\"\n",
    "\n",
    "# Verify paths\n",
    "import os\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Data not found: {DATA_PATH}\"\n",
    "assert os.path.exists(SCRIPTS_DIR), f\"Repo not cloned: {SCRIPTS_DIR}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Data: {DATA_PATH}\")\n",
    "print(f\"âœ“ Model: {BASE_MODEL}\")\n",
    "print(f\"âœ“ Output: {OUTPUT_DIR}\")\n",
    "print(f\"âœ“ Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data format\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, SCRIPTS_DIR)\n",
    "\n",
    "from load_training_data import iter_batches_raw\n",
    "\n",
    "print(\"Checking first batch...\")\n",
    "for batch in iter_batches_raw(DATA_PATH):\n",
    "    print(f\"âœ“ Loaded batch with {len(batch.samples)} samples\")\n",
    "    sample = batch.samples[0]\n",
    "    print(f\"  First sample: {sample.country} @ tick {sample.tick}\")\n",
    "    break\n",
    "print(\"âœ“ Data format verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "!cd {SCRIPTS_DIR} && python train_ai.py \\\n",
    "    --data \"{DATA_PATH}\" \\\n",
    "    --base-model \"{BASE_MODEL}\" \\\n",
    "    --output \"{OUTPUT_DIR}\" \\\n",
    "    --max-steps {MAX_STEPS} \\\n",
    "    --save-steps {SAVE_STEPS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --grad-accum {GRAD_ACCUM} \\\n",
    "    --prefetch 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output\n",
    "import os\n",
    "\n",
    "files = os.listdir(OUTPUT_DIR)\n",
    "print(f\"Adapter files in {OUTPUT_DIR}:\")\n",
    "for f in sorted(files):\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.isfile(path):\n",
    "        size = os.path.getsize(path)\n",
    "        print(f\"  {f}: {size / 1024:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"  {f}/ (checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training\n",
    "\n",
    "If Colab disconnects, resume from the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import os\n",
    "import re\n",
    "\n",
    "checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "if checkpoints:\n",
    "    latest = max(checkpoints, key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "    RESUME_FROM = os.path.join(OUTPUT_DIR, latest)\n",
    "    print(f\"Latest checkpoint: {RESUME_FROM}\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")\n",
    "    RESUME_FROM = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "if RESUME_FROM:\n",
    "    !cd {SCRIPTS_DIR} && python train_ai.py \\\n",
    "        --data \"{DATA_PATH}\" \\\n",
    "        --base-model \"{BASE_MODEL}\" \\\n",
    "        --output \"{OUTPUT_DIR}\" \\\n",
    "        --max-steps {MAX_STEPS} \\\n",
    "        --save-steps {SAVE_STEPS} \\\n",
    "        --batch-size {BATCH_SIZE} \\\n",
    "        --grad-accum {GRAD_ACCUM} \\\n",
    "        --prefetch 1000 \\\n",
    "        --resume-from \"{RESUME_FROM}\"\n",
    "else:\n",
    "    print(\"No checkpoint to resume from. Run initial training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with SmolLM2\n",
    "\n",
    "| Aspect | SmolLM2-360M | Gemma-3-270M |\n",
    "|--------|--------------|---------------|\n",
    "| Parameters | 360M | 270M |\n",
    "| Training data | 4T tokens | 6T tokens |\n",
    "| Context | 8K | 32K |\n",
    "| Architecture | LLaMA-like | Gemma-3 |\n",
    "| License | Apache 2.0 | Gemma |\n",
    "\n",
    "**Hypothesis:** Gemma-3's better training data efficiency may produce stronger EU4 decisions despite fewer parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}