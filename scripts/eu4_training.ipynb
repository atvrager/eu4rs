{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# EU4 AI Training on Colab\n\nTrain LoRA adapters for EU4 AI using CUDA GPUs.\n\n**One-time setup:**\n1. Create a GitHub PAT at https://github.com/settings/tokens (classic token, `repo` scope)\n2. In Colab sidebar: ðŸ”‘ Secrets â†’ Add new secret â†’ Name: `GITHUB_TOKEN`, Value: your PAT\n3. Create a folder in Google Drive (e.g., `eu4_training/`) for training data\n\n**Per-session:**\n1. Upload your `.cpb.zip` training data to your Drive folder\n2. Run all cells in order â€” scripts are pulled fresh from GitHub\n\n**Tip:** To get a file path, mount Drive first, then right-click file in sidebar â†’ \"Copy path\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo from GitHub (uses PAT from Colab Secrets)\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Get token from Colab Secrets (ðŸ”‘ icon in sidebar)\n",
    "token = userdata.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Clone fresh (remove old clone if exists)\n",
    "if os.path.exists(\"eu4rs\"):\n",
    "    !rm -rf eu4rs\n",
    "\n",
    "!git clone --depth 1 https://{token}@github.com/atvrager/eu4rs.git\n",
    "\n",
    "print(\"âœ“ Cloned eu4rs from GitHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and disable W&B prompts\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "!pip install -q transformers peft trl datasets pycapnp safetensors\n",
    "\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE\n",
    "# ===========================\n",
    "\n",
    "# Path to your training data in Google Drive\n",
    "# Tip: Right-click file in sidebar â†’ \"Copy path\"\n",
    "DATA_PATH = \"/content/drive/MyDrive/eu4_training/run_10yr_1.cpb.zip\"\n",
    "\n",
    "# Where to save the trained adapter (in Drive for persistence)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/eu4_training/adapters/run1\"\n",
    "\n",
    "# Model settings\n",
    "BASE_MODEL = \"HuggingFaceTB/SmolLM2-360M\"  # or \"google/gemma-2-2b-it\" for larger\n",
    "MAX_STEPS = 10000  # Adjust based on dataset size\n",
    "BATCH_SIZE = 4  # T4 handles 4-8 well\n",
    "GRAD_ACCUM = 2  # Effective batch = BATCH_SIZE * GRAD_ACCUM\n",
    "SAVE_STEPS = 2500  # Checkpoint every N steps\n",
    "\n",
    "# Repo paths (from GitHub clone)\n",
    "REPO_DIR = \"/content/eu4rs\"\n",
    "SCRIPTS_DIR = f\"{REPO_DIR}/scripts\"\n",
    "\n",
    "# Verify paths exist\n",
    "import os\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Data not found: {DATA_PATH}\"\n",
    "assert os.path.exists(SCRIPTS_DIR), f\"Repo not cloned: {SCRIPTS_DIR}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"âœ“ Data: {DATA_PATH}\")\n",
    "print(f\"âœ“ Scripts: {SCRIPTS_DIR}\")\n",
    "print(f\"âœ“ Output: {OUTPUT_DIR}\")\n",
    "print(f\"âœ“ Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data can be loaded (streaming - doesn't load full dataset)\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, SCRIPTS_DIR)\n",
    "\n",
    "from load_training_data import iter_batches_raw\n",
    "\n",
    "print(\"Checking first batch...\")\n",
    "for batch in iter_batches_raw(DATA_PATH):\n",
    "    print(f\"âœ“ Loaded batch with {len(batch.samples)} samples\")\n",
    "    sample = batch.samples[0]\n",
    "    print(f\"  First sample: {sample.country} @ tick {sample.tick}\")\n",
    "    break\n",
    "print(\"âœ“ Data format verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "!cd {SCRIPTS_DIR} && python train_ai.py \\\n",
    "    --data \"{DATA_PATH}\" \\\n",
    "    --base-model \"{BASE_MODEL}\" \\\n",
    "    --output \"{OUTPUT_DIR}\" \\\n",
    "    --max-steps {MAX_STEPS} \\\n",
    "    --save-steps {SAVE_STEPS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --grad-accum {GRAD_ACCUM} \\\n",
    "    --prefetch 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output\n",
    "import os\n",
    "\n",
    "files = os.listdir(OUTPUT_DIR)\n",
    "print(f\"Adapter files in {OUTPUT_DIR}:\")\n",
    "for f in files:\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
    "    print(f\"  {f}: {size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training\n",
    "\n",
    "If Colab disconnects, you can resume from the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import os\n",
    "import re\n",
    "\n",
    "checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "if checkpoints:\n",
    "    latest = max(checkpoints, key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "    RESUME_FROM = os.path.join(OUTPUT_DIR, latest)\n",
    "    print(f\"Latest checkpoint: {RESUME_FROM}\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")\n",
    "    RESUME_FROM = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint (run this cell to continue)\n",
    "if RESUME_FROM:\n",
    "    !cd {SCRIPTS_DIR} && python train_ai.py \\\n",
    "        --data \"{DATA_PATH}\" \\\n",
    "        --base-model \"{BASE_MODEL}\" \\\n",
    "        --output \"{OUTPUT_DIR}\" \\\n",
    "        --max-steps {MAX_STEPS} \\\n",
    "        --save-steps {SAVE_STEPS} \\\n",
    "        --batch-size {BATCH_SIZE} \\\n",
    "        --grad-accum {GRAD_ACCUM} \\\n",
    "        --prefetch 1000 \\\n",
    "        --resume-from \"{RESUME_FROM}\"\n",
    "else:\n",
    "    print(\"No checkpoint to resume from. Run initial training first.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}