{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# EU4 AI Training on Colab\n\nTrain LoRA adapters for EU4 AI using CUDA GPUs.\n\n**Setup:**\n1. Create a folder in Google Drive (e.g., `eu4_training/`)\n2. Upload your `.cpb.zip` training data to that folder\n3. Upload these files from the repo to the same folder:\n   - `scripts/train_ai.py`\n   - `scripts/load_training_data.py`\n   - `schemas/training.capnp`\n4. Run all cells in order\n\n**Tip:** To get a file path, mount Drive first, then right-click file in sidebar → \"Copy path\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and disable W&B prompts\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "!pip install -q transformers peft trl datasets pycapnp safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy scripts from Drive to local working directory\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to your scripts folder in Drive (same folder as your data)\n",
    "# Right-click folder in sidebar → \"Copy path\"\n",
    "SCRIPTS_FOLDER = \"/content/drive/MyDrive/eu4_training\"\n",
    "\n",
    "# Copy scripts to local directory\n",
    "os.makedirs(\"scripts\", exist_ok=True)\n",
    "os.makedirs(\"schemas\", exist_ok=True)  # Note: schemas/ not schema/\n",
    "\n",
    "scripts = [\"train_ai.py\", \"load_training_data.py\"]\n",
    "for script in scripts:\n",
    "    src = os.path.join(SCRIPTS_FOLDER, script)\n",
    "    dst = f\"scripts/{script}\"\n",
    "    shutil.copy(src, dst)\n",
    "    print(f\"✓ Copied {script}\")\n",
    "\n",
    "# Copy schema (load_training_data.py expects ../schemas/training.capnp)\n",
    "schema_src = os.path.join(SCRIPTS_FOLDER, \"training.capnp\")\n",
    "shutil.copy(schema_src, \"schemas/training.capnp\")\n",
    "print(\"✓ Copied training.capnp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE\n",
    "# ===========================\n",
    "\n",
    "# Path to your training data in Google Drive\n",
    "# Tip: Right-click file in sidebar → \"Copy path\"\n",
    "DATA_PATH = \"/content/drive/MyDrive/eu4_training/run_10yr_1.cpb.zip\"\n",
    "\n",
    "# Where to save the trained adapter (in Drive for persistence)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/eu4_training/adapters/run1\"\n",
    "\n",
    "# Model settings\n",
    "BASE_MODEL = \"HuggingFaceTB/SmolLM2-360M\"  # or \"google/gemma-2-2b-it\" for larger\n",
    "MAX_STEPS = 10000  # Adjust based on dataset size\n",
    "BATCH_SIZE = 4  # T4 handles 4-8 well\n",
    "GRAD_ACCUM = 2  # Effective batch = BATCH_SIZE * GRAD_ACCUM\n",
    "SAVE_STEPS = 2500  # Checkpoint every N steps\n",
    "\n",
    "# Verify paths exist\n",
    "import os\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Data not found: {DATA_PATH}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"✓ Data: {DATA_PATH}\")\n",
    "print(f\"✓ Output: {OUTPUT_DIR}\")\n",
    "print(f\"✓ Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data can be loaded (streaming - doesn't load full dataset)\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"scripts\")\n",
    "\n",
    "from load_training_data import iter_batches_raw\n",
    "\n",
    "print(\"Checking first batch...\")\n",
    "for batch in iter_batches_raw(DATA_PATH):\n",
    "    print(f\"✓ Loaded batch with {len(batch.samples)} samples\")\n",
    "    sample = batch.samples[0]\n",
    "    print(f\"  First sample: {sample.country} @ tick {sample.tick}\")\n",
    "    break\n",
    "print(\"✓ Data format verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "!cd scripts && python train_ai.py \\\n",
    "    --data \"{DATA_PATH}\" \\\n",
    "    --base-model \"{BASE_MODEL}\" \\\n",
    "    --output \"{OUTPUT_DIR}\" \\\n",
    "    --max-steps {MAX_STEPS} \\\n",
    "    --save-steps {SAVE_STEPS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --grad-accum {GRAD_ACCUM} \\\n",
    "    --prefetch 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output\n",
    "import os\n",
    "\n",
    "files = os.listdir(OUTPUT_DIR)\n",
    "print(f\"Adapter files in {OUTPUT_DIR}:\")\n",
    "for f in files:\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
    "    print(f\"  {f}: {size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Training\n",
    "\n",
    "If Colab disconnects, you can resume from the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import os\n",
    "import re\n",
    "\n",
    "checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "if checkpoints:\n",
    "    latest = max(checkpoints, key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "    RESUME_FROM = os.path.join(OUTPUT_DIR, latest)\n",
    "    print(f\"Latest checkpoint: {RESUME_FROM}\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")\n",
    "    RESUME_FROM = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint (run this cell to continue)\n",
    "if RESUME_FROM:\n",
    "    !cd scripts && python train_ai.py \\\n",
    "        --data \"{DATA_PATH}\" \\\n",
    "        --base-model \"{BASE_MODEL}\" \\\n",
    "        --output \"{OUTPUT_DIR}\" \\\n",
    "        --max-steps {MAX_STEPS} \\\n",
    "        --save-steps {SAVE_STEPS} \\\n",
    "        --batch-size {BATCH_SIZE} \\\n",
    "        --grad-accum {GRAD_ACCUM} \\\n",
    "        --prefetch 1000 \\\n",
    "        --resume-from \"{RESUME_FROM}\"\n",
    "else:\n",
    "    print(\"No checkpoint to resume from. Run initial training first.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}